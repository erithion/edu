\documentclass[a4paper,12pt,notitlepage]{article}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}

\usepackage{titling}
\usepackage{amsmath}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{hidelinks,}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{float}

\pretitle{\begin{center}\Large\bfseries}
\posttitle{\par\end{center}\vskip 0.5em}
\preauthor{\begin{center}\small}
\postauthor{\end{center}}
\predate{\centering\small}
\postdate{\par}

\title{Notes on analytic gradient derivation}
\author{Artyom Bondartsov}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
The document illustrates derivation of an analytic gradient for a neural network devised for the notorious iris dataset. It shows how neural network diagrams may be of great help when it comes to application of the chain rule correctly. Vectorized forms are also given so that the derived gradients could be used efficiently in Matlab or with linear algebra packages such as Numpy in Python. Note however that the cross entropy function used in the document does not include regularisation.
\end{abstract}

\section*{Neural network}
Having the iris dataset as input, we let $Sx$ be a set of 4-dimensional input vectors,
\begin{center}
$Sx=\begin{Bmatrix}
 ..., & \begin{bmatrix}
x_{1}\\ 
\vdots\\
x_{4} 
\end{bmatrix},&... 
\end{Bmatrix}$.
\end{center}
We denote $|Sx|$ to be the total number of such vectors in the set $Sx$.

We also have a set $Sy$ of \emph{one-hot} vectors transformed from the iris dataset's result variable with three classes, 
\begin{center}
 $Sy=\begin{Bmatrix}
 ..., & \begin{bmatrix}
y_{1}\\ 
y_{2}\\ 
y_{3} 

\end{bmatrix},&... 
\end{Bmatrix}$.
\end{center}
Please note that although $|Sy|$ is never used in the derivation that follows, yet for the sake of clarity $|Sx|=|Sy|$.

And finally we define a neural network as depicted in the figure \eqref{fig:full} below
\begin{figure}[H]
    \centering
    \includegraphics{./iris.nn.png}
%    \includegraphics[width=0.25\textwidth]{mesh}
    \caption{A neural network for the iris dataset has one input layer with 4 inputs (+bias), one hidden layer of 8 neurons (+bias) and an output layer of 3 neurons by the number of iris' classes}
    \label{fig:full}
\end{figure}
In addition we define neural network's functions as below. Note that these functions are pretty standard for neural networks and given here only as a reminder.\\\\
A sum of weighted inputs from the zero (input) layer that enters an arbitrary neuron
	\begin{equation*} z_{i}^{(1)}=w_{i,0}^{(1)}+w_{i,1}^{(1)}x_{1}+...+w_{i,4}^{(1)}x_{4}, i \in [1,8].\end{equation*} 
Using dot product, in matrix notation for the entire layer it transforms into  
      \begin{equation*}
       \begin{split} 
           \mathbf{z^{(1)}}&=\begin{bmatrix}
           &w_{1,0}^{(1)} &w_{1,1}^{(1)} &\cdots &w_{1,4}^{(1)} \\ 
           &\cdots &\cdots &\cdots &\cdots\\ 
           &w_{8,0}^{(1)} &w_{8,1}^{(1)} &\cdots &w_{8,4}^{(1)} 
       \end{bmatrix}
       \begin{bmatrix}
          1\\ 
          x_{1}\\ 
          \vdots \\ 
          x_{4}\\ 
       \end{bmatrix} \\
       &=\mathbf{w^{(1)^{T}}}\cdot \mathbf{x}.
       \end{split}
       \end{equation*}
A sigmoid function of an arbitrary neuron of the first layer
    \begin{equation*} a_{i}^{(1)}=\frac{1}{1+e^{-z_{i}^{(1)}}}, i \in [1,8]. \end{equation*}
A sum of weighted inputs from the first (hidden) layer that enters an arbitrary neuron
    \begin{equation*} z_{i}^{(2)}=w_{i,0}^{(2)}+w_{i,1}^{(2)}a_{1}^{(1)}+...+w_{i,8}^{(2)}a_{8}^{(1)}, i \in [1,3]. \end{equation*}
Using dot product, in matrix notation for the entire layer it transforms into  
       \begin{equation*}
        \begin{split}
           \mathbf{z^{(2)}}&=\begin{bmatrix}
           &w_{1,0}^{(2)} &w_{1,1}^{(2)} &\cdots &w_{1,8}^{(2)} \\ 
           &\cdots &\cdots &\cdots &\cdots\\ 
           &w_{3,0}^{(2)} &w_{3,1}^{(2)} &\cdots &w_{3,8}^{(2)} 
        \end{bmatrix}
        \begin{bmatrix}
          1\\ 
          a_{1}^{(1)}\\ 
          \vdots \\ 
          a_{8}^{(1)}
       \end{bmatrix}\\
       &=\mathbf{w^{(2)^{T}}}\cdot \mathbf{a^{(1)}}.
       \end{split}
       \end{equation*}
A sigmoid function of an arbitrary neuron of the second layer \begin{equation*} a_{i}^{(2)}=\frac{1}{1+e^{-z_{i}^{(2)}}}, i \in [1,3]. \end{equation*}
And a cross entropy loss function
\begin{equation} \label{loss} C=-\frac{1}{|Sx|}\sum_{Sx}\sum_{j=1}^{3}y_{j}\ln a_{j}^{(2)} + (1-y_{j})\ln (1-a_{j}^{(2)}). \end{equation}
Finally, we are interested in obtainig a gradient of \eqref{loss} with respect to the weights. In particular we will be deriving
\begin{align} \label{gradw2}
\nabla_{w^{(2)}}C&=\begin{bmatrix}
 \frac{\partial C}{\partial w_{1,0}^{(2)}}& \frac{\partial C}{\partial w_{1,1}^{(2)}} &\cdots &\frac{\partial C}{\partial w_{1,8}^{(2)}}\\ 
 \cdots&\cdots&\cdots&\cdots \\
 \frac{\partial C}{\partial w_{3,0}^{(2)}}& \frac{\partial C}{\partial w_{3,1}^{(2)}} &\cdots &\frac{\partial C}{\partial w_{3,8}^{(2)}}\\ 
\end{bmatrix}\\ 
\label{gradw1}
 \nabla_{w^{(1)}}C&=\begin{bmatrix}
 \frac{\partial C}{\partial w_{1,0}^{(1)}}& \frac{\partial C}{\partial w_{1,1}^{(1)}} &\cdots &\frac{\partial C}{\partial w_{1,4}^{(1)}}\\ 
 \cdots&\cdots&\cdots&\cdots \\
 \frac{\partial C}{\partial w_{8,0}^{(1)}}& \frac{\partial C}{\partial w_{8,1}^{(1)}} &\cdots &\frac{\partial C}{\partial w_{8,4}^{(1)}}\\ 
\end{bmatrix}.
\end{align}

\newpage
\section*{Gradient with respect to $\mathbf{w^{(2)}}$}
We will start from the formula \eqref{gradw2}. Yet instead of calculating the gradient matrix in its entirety we are going to derive a partial derivative in general form first, namely $\frac{\partial C}{\partial w_{i,j}^{(2)}}$.

In order to spot all parts of \eqref{loss} that depend on some arbitrary $w_{i,j}^{(2)}$ we will use a figure \eqref{fig:w2} that purposefully depicts only those dependencies
\begin{figure}[H]
    \centering
    \includegraphics{./iris.nn.w2.png}\\
%    \includegraphics[width=0.25\textwidth]{mesh}
    \caption{The diagram contains no arrows but those connecting the loss function ${C}$ to a particular weight ${w_{i,j}^{(2)}}$}
    \label{fig:w2}
\end{figure}
Now, having all the dependencies before the very eyes it is easy to write down the chain rule formula following backwards from the loss function $C$ to some arbitrary weight $w_{i,j}^{(2)}$ in the picture
\begin{equation} \label{chain2}
\begin{split}
\frac{\partial C}{\partial w_{i,j}^{(2)}}&=\frac{\partial C}{\partial a_{i}^{(2)}}\frac{\partial a_{i}^{(2)}}{\partial z_{i}^{(2)}}\frac{\partial z_{i}^{(2)}}{\partial w_{i,j}^{(2)}}.
\end{split}
\end{equation}
Taking those partial derivatives independently we get
\begin{subequations}
\begin{align}
 \frac{\partial C}{\partial a_{i}^{(2)}} 
 &  = \frac{\partial }{\partial a_{i}^{(2)}}(-\frac{1}{|Sx|}\sum_{Sx}\sum_{j=1}^{3}y_{j}\ln a_{j}^{(2)} + (1-y_{j})\ln (1-a_{j}^{(2)})) \notag \\
 & = -\frac{1}{|Sx|}\sum_{Sx}y_{i}\frac{1}{a_{i}^{(2)}}+(1-y_{i})\frac{1}{1-a_{i}^{(2)}}(-1) \notag \\
 & =\frac{1}{|Sx|}\sum_{Sx}\frac{a_{i}^{(2)}-y_{i}}{a_{i}^{(2)}(1-a_{i}^{(2)})}. \label{der2:1}\\
\frac{\partial a_{i}^{(2)}}{\partial z_{i}^{(2)}}
&= \frac{\partial }{\partial z_{i}^{(2)}}\frac{1}{1+e^{-z_{i}^{(2)}}} \notag\\
&=\frac{\partial }{\partial z_{i}^{(2)}}(1+e^{-z_{i}^{(2)}})^{-1} \notag\\
&=-(1+e^{-z_{i}^{(2)}})^{-2}(-1)e^{-z_{i}^{(2)}} \notag\\
&=\frac{e^{-z_{i}^{(2)}}}{(1+e^{-z_{i}^{(2)}})^{2}} \notag\\
&=\frac{1+e^{-z_{i}^{(2)}}-1}{(1+e^{-z_{i}^{(2)}})^{2}} \notag\\
&=\frac{\cancel{1+e^{-z_{i}^{(2)}}}}{(1+e^{-z_{i}^{(2)}})^{\cancel{2}}}-\frac{1}{(1+e^{-z_{i}^{(2)}})^{2}} \notag\\
&=\frac{1}{1+e^{-z_{i}^{(2)}}}-\frac{1}{(1+e^{-z_{i}^{(2)}})^{2}} \notag\\
&=a_{i}^{(2)}-(a_{i}^{(2)})^{2} \notag\\
&=a_{i}^{(2)}(1-a_{i}^{(2)}). \label{der2:2}\\
\frac{\partial z_{i}^{(2)}}{\partial w_{i,j}^{(2)}}
&=\frac{\partial }{\partial w_{i,j}^{(2)}}(w_{i,0}^{(2)}+w_{i,1}^{(2)}a_{1}^{(1)}+...+w_{i,j}^{(2)}a_{j}^{(1)}+...+w_{i,8}^{(2)}a_{8}^{(1)}) \notag\\
&=\left\{\begin{matrix} 1&, j=0\\ a_{j}^{(1)}&, j\neq 0. \end{matrix}\right. \label{der2:3}
\end{align}
\end{subequations}
And finally combining \eqref{der2:1}, \eqref{der2:2} and \eqref{der2:3} together we obtain
\begin{equation} \label{der2}
\begin{split}
\frac{\partial C}{\partial w_{i,j}^{(2)}}&=\frac{\partial C}{\partial a_{i}^{(2)}}\frac{\partial a_{i}^{(2)}}{\partial z_{i}^{(2)}}\frac{\partial z_{i}^{(2)}}{\partial w_{i,j}^{(2)}}\\
&=\frac{1}{|Sx|}\sum_{Sx}\frac{a_{i}^{(2)}-y_{i}}{\cancel{a_{i}^{(2)}(1-a_{i}^{(2)})}}\cancel{a_{i}^{(2)}(1-a_{i}^{(2)})}\frac{\partial z_{i}^{(2)}}{\partial w_{i,j}^{(2)}}\\
&=\frac{1}{|Sx|}\sum_{Sx}(a_{i}^{(2)}-y_{i})\left\{\begin{matrix} 1&, j=0\\ a_{j}^{(1)}&, j\neq 0 \end{matrix}\right.\\
&=\left\{\begin{matrix} \frac{1}{|Sx|}\sum_{Sx}(a_{i}^{(2)}-y_{i})&, j=0\\ \frac{1}{|Sx|}\sum_{Sx}(a_{i}^{(2)}-y_{i})a_{j}^{(1)}&, j\neq 0. \end{matrix}\right.
\end{split}
\end{equation}
Using \eqref{der2} our matrix gradient \eqref{gradw2} now takes the form
\begin{equation} \label{gradw2f}
\begin{split}
\nabla_{w^{(2)}}C&=\begin{bmatrix}
 \frac{\partial C}{\partial w_{1,0}^{(2)}}& \frac{\partial C}{\partial w_{1,1}^{(2)}} &\cdots &\frac{\partial C}{\partial w_{1,8}^{(2)}}\\ 
 \cdots&\cdots&\cdots&\cdots \\
 \frac{\partial C}{\partial w_{3,0}^{(2)}}& \frac{\partial C}{\partial w_{3,1}^{(2)}} &\cdots &\frac{\partial C}{\partial w_{3,8}^{(2)}}\\ 
\end{bmatrix}\\ 
&=\frac{1}{|Sx|}\begin{bmatrix}
 \sum_{Sx}(a_{1}^{(2)}-y_{1}) &\sum_{Sx}(a_{1}^{(2)}-y_{1})a_{1}^{(1)}  &...  &\sum_{Sx}(a_{1}^{(2)}-y_{1})a_{8}^{(1)} \\ 
 ...&...  &... &... \\ 
 \sum_{Sx}(a_{3}^{(2)}-y_{3}) &\sum_{Sx}(a_{3}^{(2)}-y_{3})a_{1}^{(1)}  &...  &\sum_{Sx}(a_{3}^{(2)}-y_{3})a_{8}^{(1)}
\end{bmatrix}.
\end{split}
\end{equation}

In addition, \eqref{gradw2f} can also take a nice vectorized form using dot product
\begin{equation} \label{gradw2f2}
\begin{split}
\nabla_{w^{(2)}}C&=\frac{1}{|Sx|}\begin{bmatrix}
 a_{1,1}^{(2)}-y_{1,1}& \cdots &a_{1,|Sx|}^{(2)}-y_{1,|Sx|} \\ 
 \cdots& \cdots &\cdots \\
 a_{3,1}^{(2)}-y_{3,1}& \cdots &a_{3,|Sx|}^{(2)}-y_{1,|Sx|} \\
\end{bmatrix}\begin{bmatrix}
 1& a_{1,1}^{(1)} & \cdots &a_{1,8}^{(1)} \\ 
 \vdots& \vdots & \cdots & \vdots \\ 
 1& a_{|Sx|,1}^{(1)} & \cdots  & a_{|Sx|,8}^{(1)}
\end{bmatrix}\\
&=\frac{1}{|Sx|}(\mathbf{a^{(2)}}-\mathbf{y})\cdot\mathbf{a^{(1)^{T}}}.
\end{split}
\end{equation}
Please note that $\mathbf{a^{(1)^{T}}}$ includes a bias first column.
\newpage 
\section*{Gradient with respect to $\mathbf{w^{(1)}}$}
In order to obtain \eqref{gradw1} we will use the same approach, i.e. we will derive a general formula of partial derivative, namely $\frac{\partial C}{\partial w_{i,j}^{(1)}}$.

Once again we are going to employ graphical approach to spot all parts of \eqref{loss} that would get changed if some arbitrary $w_{i,j}^{(1)}$ was wiggled.
\begin{figure}[H]
    \centering
    \includegraphics{./iris.nn.w1.png}
%    \includegraphics[width=0.25\textwidth]{mesh}
    \caption{This time from the loss function ${C}$ to a particular weight ${w_{i,j}^{(1)}}$ leads multiple paths}
    \label{fig:w1}
\end{figure}
Starting from $C$ in figure \eqref{fig:w1} and going backwards to the arbitrary weight $w_{i,j}^{(1)}$ we write down
\begin{equation} \label{chain1}
\begin{split}
\frac{\partial C}{\partial w_{i,j}^{(1)}}
&=\frac{\partial C}{\partial a_{1}^{(2)}}\frac{\partial a_{1}^{(2)}}{\partial z_{1}^{(2)}}\frac{\partial z_{1}^{(2)}}{\partial a_{i}^{(1)}}\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}\frac{\partial z_{i}^{(1)}}{\partial w_{i,j}^{(1)}}
+\frac{\partial C}{\partial a_{2}^{(2)}}\frac{\partial a_{2}^{(2)}}{\partial z_{2}^{(2)}}\frac{\partial z_{2}^{(2)}}{\partial a_{i}^{(1)}}\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}\frac{\partial z_{i}^{(1)}}{\partial w_{i,j}^{(1)}}\\
&+\frac{\partial C}{\partial a_{3}^{(2)}}\frac{\partial a_{3}^{(2)}}{\partial z_{3}^{(2)}}\frac{\partial z_{3}^{(2)}}{\partial a_{i}^{(1)}}\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}\frac{\partial z_{i}^{(1)}}{\partial w_{i,j}^{(1)}}\\
&=\left( \sum_{k=1}^{3}\frac{\partial C}{\partial a_{k}^{(2)}}\frac{\partial a_{k}^{(2)}}{\partial z_{k}^{(2)}}\frac{\partial z_{k}^{(2)}}{\partial a_{i}^{(1)}}\right )\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}\frac{\partial z_{i}^{(1)}}{\partial w_{i,j}^{(1)}}.
\end{split}
\end{equation}
Please note how the multiple paths in the picture have been transformed into summation in the formula. The chain rule \eqref{chain1} looks scary yet let us recall that we have already carried out derivations for some of these partial derivatives. Specifically, $\frac{\partial C}{\partial a_{k}^{(2)}}$ is \eqref{der2:1} and $\frac{\partial a_{k}^{(2)}}{\partial z_{k}^{(2)}}$ is \eqref{der2:2}. Hence only 3 partial derivatives left, and one of them, $\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}$, is in fact also \eqref{der2:2} but with a different upper index. Thus, taking care of the rest of the derivatives independently we get
\begin{subequations}
\begin{align}
\frac{\partial z_{k}^{(2)}}{\partial a_{i}^{(1)}}
 & = \frac{\partial }{\partial a_{i}^{(1)}}\left(w_{k,0}^{(2)}+w_{k,1}^{(2)}a_{1}^{(1)}+...+w_{k,i}^{(2)}a_{i}^{(1)}+...+w_{k,8}^{(2)}a_{8}^{(1)} \right )  \notag \\
 & = w_{k,i}^{(2)}. \label{der1:1}\\
\frac{\partial z_{i}^{(1)}}{\partial w_{i,j}^{(1)}}
 & = \frac{\partial }{\partial w_{i,j}^{(1)}}\left(w_{i,0}^{(1)}+w_{i,1}^{(1)}x_{1}+...+w_{i,j}^{(1)}x_{j}+...+w_{i,4}^{(1)}x_{4} \right ) \notag \\
 & =  \left\{\begin{matrix}
1&, j = 0\\ 
x_{j}&, j \neq 0
\end{matrix}\right.. \label{der1:2}\\
\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}
 & =a_{i}^{(1)}(1-a_{i}^{(1)}). \label{der1:3}
\end{align}
\end{subequations}
Now combining \eqref{der2:1}, \eqref{der2:2}, \eqref{der1:1}, \eqref{der1:2} and \eqref{der1:3} we have
\begin{equation} \label{der1}
\begin{split}
\frac{\partial C}{\partial w_{i,j}^{(1)}}
&=\left( \sum_{k=1}^{3}\frac{\partial C}{\partial a_{k}^{(2)}}\frac{\partial a_{k}^{(2)}}{\partial z_{k}^{(2)}}\frac{\partial z_{k}^{(2)}}{\partial a_{i}^{(1)}}\right )\frac{\partial a_{i}^{(1)}}{\partial z_{i}^{(1)}}\frac{\partial z_{i}^{(1)}}{\partial w_{i,j}^{(1)}}\\
&=\left(\sum_{k=1}^{3} \frac{1}{|Sx|}\sum_{Sx} \frac{a_{k}^{(2)}-y_{k}}{\cancel{a_{k}^{(2)}(1-a_{k}^{(2)})}}\cancel{a_{k}^{(2)}(1-a_{k}^{(2)})}w_{k,i}^{(2)}\right )a_{i}^{(1)}(1-a_{i}^{(1)})\left\{\begin{matrix}
1&, j = 0\\ 
x_{j}&, j \neq 0
\end{matrix}\right.\\
&=\frac{1}{|Sx|}\sum_{Sx}\left(\sum_{k=1}^{3} (a_{k}^{(2)}-y_{k})w_{k,i}^{(2)}\right )a_{i}^{(1)}(1-a_{i}^{(1)})\left\{\begin{matrix}
1&, j = 0\\ 
x_{j}&, j \neq 0
\end{matrix}\right.\\
&=\left\{\begin{matrix}
\frac{1}{|Sx|}\sum_{Sx}\left(\sum_{k=1}^{3} (a_{k}^{(2)}-y_{k})w_{k,i}^{(2)}\right )a_{i}^{(1)}(1-a_{i}^{(1)})&, j = 0\\ 
\frac{1}{|Sx|}\sum_{Sx}\left(\sum_{k=1}^{3} (a_{k}^{(2)}-y_{k})w_{k,i}^{(2)}\right )a_{i}^{(1)}(1-a_{i}^{(1)})x_{j}&, j \neq 0
\end{matrix}\right..
\end{split}
\end{equation}
Before proceeding to the matrix gradient \eqref{gradw1} we are going to introduce some additional notation for the sake of simplification. We will denote
\begin{subequations}
\begin{align}
f_{i}^{(2)}&=\left(\sum_{k=1}^{3} (a_{k}^{(2)}-y_{k})w_{k,i}^{(2)}\right ), i \in [1,8] \label{den:1}\\
g_{i}^{(1)}&=a_{i}^{(1)}(1-a_{i}^{(1)}), i \in [1,8]. \label{den:2}
\end{align}
\end{subequations}
Using \eqref{den:1} and \eqref{den:2} the matrix gradient \eqref{gradw1} now takes the form
\begin{align} \label{gradw1f}
\nabla_{w^{(1)}}C
&=\begin{bmatrix}
 \frac{\partial C}{\partial w_{1,0}^{(1)}}& \frac{\partial C}{\partial w_{1,1}^{(1)}} &\cdots &\frac{\partial C}{\partial w_{1,4}^{(1)}}\\ 
 \cdots&\cdots&\cdots&\cdots \\
 \frac{\partial C}{\partial w_{8,0}^{(1)}}& \frac{\partial C}{\partial w_{8,1}^{(1)}} &\cdots &\frac{\partial C}{\partial w_{8,4}^{(1)}}\\ 
 \end{bmatrix} \notag \\
&=\frac{1}{|Sx|}\begin{bmatrix}
 \sum_{Sx}g_{1}^{(1)}f_{1}^{(2)}&\sum_{Sx}g_{1}^{(1)}f_{1}^{(2)}x_{1}  &\cdots  &\sum_{Sx}g_{1}^{(1)}f_{1}^{(2)}x_{4} \\ 
 \cdots & \cdots & \cdots & \cdots\\ 
 \sum_{Sx}g_{8}^{(1)}f_{8}^{(2)}& \sum_{Sx}g_{8}^{(1)}f_{8}^{(2)}x_{1} & \cdots & \sum_{Sx}g_{8}^{(1)}f_{8}^{(2)}x_{4}. 
\end{bmatrix}
\end{align}

In addition, \eqref{gradw1f} can also take a nice vectorized form using dot product
\begin{equation} \label{gradw1f2}
\begin{split}
\nabla_{w^{(1)}}C
&=\frac{1}{|Sx|}\begin{bmatrix}
 g_{1,1}^{(1)}f_{1,1}^{(2)}& \cdots & g_{1,|Sx|}^{(1)}f_{1,|Sx|}^{(2)}\\ 
 \cdots& \cdots & \cdots\\ 
 g_{8,1}^{(1)}f_{8,1}^{(2)}& \cdots & g_{8,|Sx|}^{(1)}f_{8,|Sx|}^{(2)}
\end{bmatrix}\cdot\begin{bmatrix}
 1& x_{1,1} & \cdots & x_{1,4}\\ 
 \vdots& \vdots & \vdots & \vdots \\ 
 1& x_{|Sx|,1} & \cdots & x_{|Sx|,4}
\end{bmatrix}\\
&=\frac{1}{|Sx|}\left(\mathbf{g^{(1)}}\odot\mathbf{f^{(2)}} \right )\cdot \mathbf{x^{T}},
\end{split}
\end{equation}
where $\odot$ is Hadamard (element-wise) product.

Please note that $\mathbf{x^{T}}$ includes a bias first column. Heed that \eqref{den:1} can aslo be vectorized if needed
\begin{equation}
\begin{split}
\mathbf{f^{(2)}}
&=\begin{bmatrix}
 a_{1,1}^{(2)}-y_{1,1}& \cdots & a_{1,3}^{(2)}-y_{1,3}\\ 
 \cdots &\cdots  & \cdots\\ 
 a_{|Sx|,1}^{(2)}-y_{|Sx|,1}& \cdots & a_{|Sx|,3}^{(2)}-y_{|Sx|,3}
\end{bmatrix}
\begin{bmatrix}
 w_{1,1}^{(2)}& \cdots & w_{1,8}^{(2)}\\ 
 \vdots& \vdots & \vdots\\ 
 w_{3,1}^{(2)}& \cdots & w_{3,8}^{(2)}
\end{bmatrix}\\
&=(\mathbf{a^{(2)}-y})^{\textup{T}}\cdot\mathbf{w^{(2)}}.
\end{split}
\end{equation}


\end{document}
